{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ff150e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uproot as u\n",
    "import numpy as np\n",
    "import pyhf\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c333d47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test option\n",
    "tests = True\n",
    "\n",
    "flat_uncertainty=False\n",
    "uncertainty = 0.5\n",
    "nominal_eps = 1e-3 \n",
    "obs_eps = []\n",
    "\n",
    "# Array to contain expected values \n",
    "obs_eps = []\n",
    "exp_eps = []\n",
    "exp_minus_two = []\n",
    "exp_minus_one = []\n",
    "exp_plus_one = []\n",
    "exp_plus_two = []\n",
    "\n",
    "scaling_list = [] \n",
    "dm_type = \"fermion\"\n",
    "alpha = 0.1\n",
    "ratio = \"0.6\"\n",
    "tag = \"CNN\"\n",
    "\n",
    "\n",
    "if(alpha==1.0):\n",
    "    scaling = 0.01\n",
    "else:\n",
    "    scaling = 0.01\n",
    "\n",
    "\n",
    "base_dir_run1 = \"/home/lmlepin/Desktop/dm_sets/dark_tridents_analysis/run1_samples/\"\n",
    "base_dir_run3 = \"/home/lmlepin/Desktop/dm_sets/dark_tridents_analysis/run3_samples/\"\n",
    "\n",
    "signal_dir_run1 = \"/home/lmlepin/Desktop/dm_sets/dark_tridents_analysis/run1_signal/\"\n",
    "signal_dir_run3 = \"/home/lmlepin/Desktop/dm_sets/dark_tridents_analysis/run3_signal/\"\n",
    "\n",
    "\n",
    "signal_run1 = u.open(signal_dir_run1 + dm_type + \"_ratio_\" + ratio + \"_signal_hist_run1_\" + tag + \".root\")\n",
    "signal_run3 = u.open(signal_dir_run3 + dm_type + \"_ratio_\" + ratio + \"_signal_hist_run3_\" + tag + \".root\")\n",
    "\n",
    "bkg_run1 = u.open(base_dir_run1 + \"background_hist_run1_\" + tag + \".root\")\n",
    "bkg_run3 = u.open(base_dir_run3 + \"background_hist_run3_\" + tag + \".root\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "337cd29b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Background events run1: 241.03\n",
      "Background events run3: 463.23\n",
      "\n",
      "\n",
      "Data events run1: 247.00\n",
      "Data events run3: 394.00\n"
     ]
    }
   ],
   "source": [
    "masses = [\"0.01\", \"0.02\", \"0.03\", \"0.04\", \"0.05\", \"0.06\", \"0.07\", \"0.08\", \"0.09\", \"0.1\", \"0.2\", \"0.3\", \"0.4\"]\n",
    "\n",
    "print(\"Background events run1: {nevts:.2f}\".format( nevts= np.sum(bkg_run1[\"bkg_total_hist\"].values())))\n",
    "print(\"Background events run3: {nevts:.2f}\".format( nevts= np.sum(bkg_run3[\"bkg_total_hist\"].values()))) \n",
    "print(\"\\n\")\n",
    "print(\"Data events run1: {nevts:.2f}\".format( nevts= np.sum(bkg_run1[\"data_hist\"].values())))\n",
    "print(\"Data events run3: {nevts:.2f}\".format( nevts= np.sum(bkg_run3[\"data_hist\"].values()))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1adda34d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if(not tests):\n",
    "\n",
    "\n",
    "    delimiter=0\n",
    "    pot_uncert = 0.02 # 2% POT uncertainty \n",
    "\n",
    "\n",
    "    total_bkg = np.sum(bkg_run1[\"bkg_total_hist\"].values()) + np.sum(bkg_run3[\"bkg_total_hist\"].values())\n",
    "\n",
    "    total_run1 = np.sum(bkg_run1[\"bkg_total_hist\"].values())\n",
    "    total_run3 = np.sum(bkg_run3[\"bkg_total_hist\"].values())\n",
    "\n",
    "    n_back = bkg_run1[\"bkg_total_hist\"].values().tolist()[delimiter:]\n",
    "    n_back.extend(bkg_run3[\"bkg_total_hist\"].values().tolist()[delimiter:])\n",
    "\n",
    "\n",
    "    n_data = bkg_run1[\"data_hist\"].values().tolist()[delimiter:]\n",
    "    n_data.extend(bkg_run3[\"data_hist\"].values().tolist()[delimiter:])\n",
    "\n",
    "\n",
    "    if(flat_uncertainty):\n",
    "        back_sigma =  (bkg_run1[\"bkg_total_hist\"].values()*uncertainty).tolist()\n",
    "        back_sigma.extend((bkg_run3[\"bkg_total_hist\"].values()*uncertainty).tolist())\n",
    "    else:\n",
    "        back_sigma =  (bkg_run1[\"bkg_total_uncert\"].values()).tolist()[delimiter:]\n",
    "        back_sigma.extend((bkg_run3[\"bkg_total_uncert\"].values()).tolist()[delimiter:])\n",
    "\n",
    "    if alpha == 1.0:\n",
    "        scaling_a1 = (1./pow(0.1,3))\n",
    "    else:\n",
    "        scaling_a1 = 1. \n",
    "\n",
    "    for mass in masses:\n",
    "\n",
    "        print(\"Processing mass: \" + mass)\n",
    "        print(\"Signal events run1: {nevts:.5f}\".format( nevts= np.sum(signal_run1[\"signal_\"+ mass].values()*scaling_a1)))\n",
    "        print(\"Signal events run3: {nevts:.5f}\".format( nevts= np.sum(signal_run3[\"signal_\"+ mass].values()*scaling_a1)))\n",
    "\n",
    "        total_sig = np.sum(signal_run1[\"signal_\"+mass].values()*scaling_a1) + np.sum(signal_run3[\"signal_\"+mass].values()*scaling_a1)\n",
    "        total_sig_run1 = np.sum(signal_run1[\"signal_\"+mass].values()*scaling_a1)\n",
    "        total_sig_run3 =  np.sum(signal_run3[\"signal_\"+mass].values()*scaling_a1)\n",
    "        factor = scaling*(((total_run1/total_sig_run1)+ (total_run3/total_sig_run3))/2.)\n",
    "        total_sig_adjusted = np.sum(signal_run1[\"signal_\"+mass].values()*scaling_a1*factor) + np.sum(signal_run3[\"signal_\"+mass].values()*scaling_a1*factor)\n",
    "        scaling_list.append(factor)\n",
    "\n",
    "        print(\"Scaling factor: \", factor)\n",
    "\n",
    "\n",
    "        # Factor scalings is introduced to have a signal strength between 0 and 10\n",
    "        n_sig_run1 = (signal_run1[\"signal_\"+mass].values()*scaling_a1*factor).tolist()[delimiter:]\n",
    "        n_sig_run3 = (signal_run3[\"signal_\"+mass].values()*scaling_a1*factor).tolist()[delimiter:]\n",
    "        n_sig_run1.extend(n_sig_run3)\n",
    "\n",
    "\n",
    "        # Total signal errors are in %, need to convert them to absolute errors for shapesys modifier  \n",
    "        sigma_sig_run1 = (signal_run1[\"signal_total_error_\"+mass].values()/100.)*(signal_run1[\"signal_\"+mass].values()*scaling_a1*factor)\n",
    "        sigma_sig_run3 = (signal_run3[\"signal_total_error_\"+mass].values()/100.)*(signal_run3[\"signal_\"+mass].values()*scaling_a1*factor)\n",
    "\n",
    "\n",
    "        sigma_sig_run1 = (sigma_sig_run1).tolist()[delimiter:]\n",
    "        sigma_sig_run3 = (sigma_sig_run3).tolist()[delimiter:]\n",
    "        sigma_sig_run1.extend(sigma_sig_run3)\n",
    "\n",
    "\n",
    "\n",
    "        model = pyhf.Model(\n",
    "            {\n",
    "          \"channels\": [\n",
    "            {\n",
    "              \"name\": \"singlechannel\",\n",
    "              \"samples\": [\n",
    "                {\n",
    "                  \"name\": \"signal\",\n",
    "                  \"data\": n_sig_run1,\n",
    "                  \"modifiers\": [\n",
    "                    {\"name\": \"mu\", \"type\": \"normfactor\", \"data\": None}, #This is the scaling which is to be calculated\n",
    "                    {\"name\": \"uncorr_siguncrt\", \"type\": \"shapesys\", \"data\": sigma_sig_run1},\n",
    "                    {\"name\": \"pot_correlaated\", \"type\": \"normsys\", \"data\": {\"hi\":1.02, \"lo\":0.98}},\n",
    "                  ]\n",
    "                },\n",
    "                {\n",
    "                  \"name\": \"background\",\n",
    "                  \"data\": n_back,\n",
    "                  \"modifiers\": [\n",
    "                    {\"name\": \"uncorr_bkguncrt\", \"type\": \"shapesys\", \"data\": back_sigma},\n",
    "                    {\"name\": \"pot_correlated\", \"type\": \"normsys\", \"data\": {\"hi\":1.02, \"lo\":0.98}},\n",
    "                  ]\n",
    "                }\n",
    "              ]\n",
    "            }\n",
    "          ]\n",
    "        }\n",
    "        )\n",
    "\n",
    "        obs = n_data + model.config.auxdata\n",
    "\n",
    "        poi_values = np.linspace(0., 10., 100)\n",
    "        obs_limit, exp_limits, (scan, results) = pyhf.infer.intervals.upperlimit(\n",
    "            obs, model, poi_values, level=0.1, return_results=True)\n",
    "        print(f\"Upper limit (obs): μ = {obs_limit:.4f}\")\n",
    "        print(f\"Upper limit (exp): μ = {exp_limits[2]:.4f}\")\n",
    "\n",
    "        obs_epsilon = (nominal_eps**2)*np.sqrt(obs_limit*factor)\n",
    "        exp_epsilon = (nominal_eps**2)*np.sqrt(exp_limits[2]*factor)\n",
    "        exp_two_down = (nominal_eps**2)*np.sqrt(exp_limits[0]*factor)\n",
    "        exp_one_down = (nominal_eps**2)*np.sqrt(exp_limits[1]*factor)\n",
    "        exp_one_up = (nominal_eps**2)*np.sqrt(exp_limits[3]*factor)\n",
    "        exp_two_up = (nominal_eps**2)*np.sqrt(exp_limits[4]*factor)\n",
    "\n",
    "        obs_eps.append(obs_epsilon)\n",
    "        exp_eps.append(exp_epsilon)\n",
    "        exp_minus_two.append(exp_two_down)\n",
    "        exp_minus_one.append(exp_one_down)\n",
    "        exp_plus_one.append(exp_one_up)\n",
    "        exp_plus_two.append(exp_two_up)\n",
    "        print(f\"Upper limit (obs): epsilon2 = {obs_epsilon}\")\n",
    "        print(f\"Upper limit (exp): epsilon2 = {exp_epsilon}\")\n",
    "        print(f\"Upper limit +2sigma: epsilon2 = {exp_two_up}\")\n",
    "        print(f\"Upper limit +1sigma: epsilon2 = {exp_one_up}\")\n",
    "        print(f\"Upper limit -1sigma: epsilon2 = {exp_one_down}\")\n",
    "        print(f\"Upper limit -2sigma: epsilon2 = {exp_two_down}\")\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5eb0d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if(not tests):\n",
    "\n",
    "\n",
    "    import pandas as pd\n",
    "    from matplotlib import patheffects\n",
    "\n",
    "    limits_dir = \"/home/lmlepin/Desktop/dm_sets/dark_tridents_analysis/\"\n",
    "    plots_dir = \"/home/lmlepin/Desktop/Plots_DT_Drive/2023/sensitivity_plots/\"\n",
    "    sensitivity_files = \"/home/lmlepin/Desktop/dm_sets/dark_tridents_analysis/sensitivity_files/\"\n",
    "\n",
    "    df = pd.read_csv(limits_dir + \"dark_tridents_current_limits.csv\")\n",
    "    df_b = pd.read_csv(limits_dir + \"dark_tridents_current_limits_1.csv\")\n",
    "    df_babar = pd.read_csv(limits_dir + \"babar_paper.csv\")\n",
    "    df_na = pd.read_csv(limits_dir + \"NA48_2.csv\")\n",
    "\n",
    "\n",
    "    dp_masses = [0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1, 0.2, 0.3, 0.4]\n",
    "\n",
    "\n",
    "    plt.figure(figsize=(17,14),dpi=300)\n",
    "    plt.axis([ 1e-2, 1, 1e-11, 1e-5])\n",
    "    plt.plot(dp_masses,exp_eps,label=r'MicroBooNE',color='red')\n",
    "    plt.plot(dp_masses,obs_eps,label=r'MicroBooNE',color='black')\n",
    "\n",
    "\n",
    "\n",
    "    if(dm_type == 'fermion' and alpha == 0.1):\n",
    "        plt.plot(df['X_LSND'],df['Y_LSND'],'-', label=r'LSND',color='green')\n",
    "        plt.fill_between(df['X_LSND'],df['Y_LSND'], 1e-5,color='green',alpha=0.2)\n",
    "        plt.plot(df['X_PLANCK'],df['Y_PLANCK'],'-',color='orange', path_effects=[patheffects.withTickedStroke(spacing=10, angle=280)])\n",
    "        plt.plot(df['X_PLANCK'],df['Y_PLANCK'],'-', label=r'Planck (fermionic DM)',color='orange')\n",
    "    elif(dm_type == 'fermion' and alpha == 1.):\n",
    "        plt.plot(df_b['X_LSND'],df_b['Y_LSND'],'-', label=r'LSND',color='green')\n",
    "        plt.fill_between(df_b['X_LSND'],df_b['Y_LSND'], 1e-5,color='green',alpha=0.2)\n",
    "        plt.plot(df_b['X_PLANCK'],df_b['Y_PLANCK'],'-',color='orange', path_effects=[patheffects.withTickedStroke(spacing=10, angle=280)])\n",
    "        plt.plot(df_b['X_PLANCK'],df_b['Y_PLANCK'],'-', label=r'Planck (fermionic DM)',color='orange')\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "\n",
    "    limit_dict = {'mass':dp_masses,'observed':obs_eps, 'epsilon_squared':exp_eps, 'two_sig_down':exp_minus_two,\n",
    "                 'one_sig_down':exp_minus_one, 'one_sig_up':exp_plus_one, 'two_sig_up':exp_plus_two}\n",
    "    df_out = pd.DataFrame.from_dict(limit_dict)\n",
    "    df_out.to_csv(sensitivity_files + dm_type + \"_\" + tag + \"_sensitivity_alpha_\" + str(alpha) + \"_all_runs_full_uncert_signal_flux_90_percent.csv\")\n",
    "\n",
    "\n",
    "    #plt.plot(df['X_BD'],df['Y_BD'],'-', label=r'Beam Dump',color='mediumpurple')\n",
    "    plt.fill_between(df['X_BD'],df['Y_BD'],1e-11,color='mediumpurple',alpha=0.3)\n",
    "    #plt.plot(df_babar['X_babar'],np.square(df_babar['Y_babar']),'-', label=r'BaBar',color='blue')\n",
    "    plt.fill_between(df_babar['X_babar'],np.square(df_babar['Y_babar']), 1e-5,color='blue',alpha=0.2)\n",
    "    #plt.plot(df_na['X']*1e-3,df_na['Y'],'-', label=r'NA48/2',color='blue')\n",
    "    plt.fill_between(df_na['X']*1e-3,df_na['Y'], 1e-5,color='blue',alpha=0.3)\n",
    "    plt.yscale('log')\n",
    "    plt.xscale('log')\n",
    "    plt.ylim(1e-11,1e-5)\n",
    "    plt.xlim(1e-2,5e-1)\n",
    "    plt.legend(fontsize=20,loc=\"lower right\",shadow=True)\n",
    "    plt.xticks(size=25)\n",
    "    plt.yticks(size=25)\n",
    "    plt.title(\"Sensitivity for 7.56e20 POT           \" + dm_type + \" DM: \" + r'$\\alpha_{D}$ = ' + str(alpha), size = 30, pad=20)\n",
    "    plt.xlabel(r'$M_{A^{\\prime}}$[GeV]',size=30, labelpad=20)\n",
    "    plt.ylabel(r'$\\epsilon^2$',size=30, labelpad=20)\n",
    "    #plt.minorticks_off()\n",
    "    #plt.savefig(plots_dir + dm_type + \"_\" + tag + \"_sensitivity_alpha_\" + str(alpha) + \".png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25bfe8a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing mass: 0.05\n",
      "Signal events run1: 68.88382\n",
      "Signal events run3: 173.19703\n",
      "Scaling factor:  0.030868031917079757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lmlepin/anaconda3/envs/dark_trident/lib/python3.7/site-packages/pyhf/infer/calculators.py:369: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  teststat = (qmu - qmu_A) / (2 * self.sqrtqmuA_v)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing upper limits results...\n",
      "Upper limit (obs): μ = 4.4971\n",
      "Upper limit (exp): μ = 5.0139\n",
      "Upper limit (obs): epsilon2 = 3.725816575619989e-07\n",
      "Upper limit (exp): epsilon2 = 3.934091324456531e-07\n",
      "Upper limit +2sigma: epsilon2 = 5.555900639597487e-07\n",
      "Upper limit +1sigma: epsilon2 = 4.76194750310008e-07\n",
      "Upper limit -1sigma: epsilon2 = 3.2768435013372346e-07\n",
      "Upper limit -2sigma: epsilon2 = 2.795400126559571e-07\n",
      "\n",
      "\n",
      "Printing CLs values for the observed value of mu (Asymptotic)...\n",
      "Observed CLs = 0.06918225141258759\n",
      "Expected CLs = [array(0.00587538), array(0.02574035), array(0.09995778), array(0.30836906), array(0.65355174)]\n",
      "\n",
      "\n",
      "Printing CLs values for the observed value of mu (Toys)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Signal-like:  75%|██████████████████▋      | 1495/2000 [10:06<03:20,  2.52toy/s]/home/lmlepin/anaconda3/envs/dark_trident/lib/python3.7/site-packages/scipy/optimize/optimize.py:283: RuntimeWarning: Values in x were outside bounds during a minimize step, clipping to bounds\n",
      "  \"minimize step, clipping to bounds\", RuntimeWarning)\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observed CLs = 0.07198952879581151\n",
      "Expected CLs = [array(0.), array(0.03835732), array(0.11844093), array(0.35050584), array(1.)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# Comparison with toy-based calculator \n",
    "\n",
    "\n",
    "if(tests):\n",
    "    masses = [\"0.05\"]\n",
    "    delimiter=0\n",
    "    pot_uncert = 0.02 # 2% POT uncertainty \n",
    "\n",
    "\n",
    "    total_bkg = np.sum(bkg_run1[\"bkg_total_hist\"].values()) + np.sum(bkg_run3[\"bkg_total_hist\"].values())\n",
    "\n",
    "    total_run1 = np.sum(bkg_run1[\"bkg_total_hist\"].values())\n",
    "    total_run3 = np.sum(bkg_run3[\"bkg_total_hist\"].values())\n",
    "\n",
    "    n_back = bkg_run1[\"bkg_total_hist\"].values().tolist()[delimiter:]\n",
    "    n_back.extend(bkg_run3[\"bkg_total_hist\"].values().tolist()[delimiter:])\n",
    "\n",
    "\n",
    "    n_data = bkg_run1[\"data_hist\"].values().tolist()[delimiter:]\n",
    "    n_data.extend(bkg_run3[\"data_hist\"].values().tolist()[delimiter:])\n",
    "\n",
    "\n",
    "    if(flat_uncertainty):\n",
    "        back_sigma =  (bkg_run1[\"bkg_total_hist\"].values()*uncertainty).tolist()\n",
    "        back_sigma.extend((bkg_run3[\"bkg_total_hist\"].values()*uncertainty).tolist())\n",
    "    else:\n",
    "        back_sigma =  (bkg_run1[\"bkg_total_uncert\"].values()).tolist()[delimiter:]\n",
    "        back_sigma.extend((bkg_run3[\"bkg_total_uncert\"].values()).tolist()[delimiter:])\n",
    "\n",
    "    if alpha == 1.0:\n",
    "        scaling_a1 = (1./pow(0.1,3))\n",
    "    else:\n",
    "        scaling_a1 = 1. \n",
    "\n",
    "    for mass in masses:\n",
    "\n",
    "        print(\"Processing mass: \" + mass)\n",
    "        print(\"Signal events run1: {nevts:.5f}\".format( nevts= np.sum(signal_run1[\"signal_\"+ mass].values()*scaling_a1)))\n",
    "        print(\"Signal events run3: {nevts:.5f}\".format( nevts= np.sum(signal_run3[\"signal_\"+ mass].values()*scaling_a1)))\n",
    "\n",
    "        total_sig = np.sum(signal_run1[\"signal_\"+mass].values()*scaling_a1) + np.sum(signal_run3[\"signal_\"+mass].values()*scaling_a1)\n",
    "        total_sig_run1 = np.sum(signal_run1[\"signal_\"+mass].values()*scaling_a1)\n",
    "        total_sig_run3 =  np.sum(signal_run3[\"signal_\"+mass].values()*scaling_a1)\n",
    "        factor = scaling*(((total_run1/total_sig_run1)+ (total_run3/total_sig_run3))/2.)\n",
    "        total_sig_adjusted = np.sum(signal_run1[\"signal_\"+mass].values()*scaling_a1*factor) + np.sum(signal_run3[\"signal_\"+mass].values()*scaling_a1*factor)\n",
    "        scaling_list.append(factor)\n",
    "\n",
    "        print(\"Scaling factor: \", factor)\n",
    "\n",
    "\n",
    "        # Factor scalings is introduced to have a signal strength between 0 and 10\n",
    "        n_sig_run1 = (signal_run1[\"signal_\"+mass].values()*scaling_a1*factor).tolist()[delimiter:]\n",
    "        n_sig_run3 = (signal_run3[\"signal_\"+mass].values()*scaling_a1*factor).tolist()[delimiter:]\n",
    "        n_sig_run1.extend(n_sig_run3)\n",
    "\n",
    "\n",
    "        # Total signal errors are in %, need to convert them to absolute errors for shapesys modifier  \n",
    "        sigma_sig_run1 = (signal_run1[\"signal_total_error_\"+mass].values()/100.)*(signal_run1[\"signal_\"+mass].values()*scaling_a1*factor)\n",
    "        sigma_sig_run3 = (signal_run3[\"signal_total_error_\"+mass].values()/100.)*(signal_run3[\"signal_\"+mass].values()*scaling_a1*factor)\n",
    "\n",
    "\n",
    "        sigma_sig_run1 = (sigma_sig_run1).tolist()[delimiter:]\n",
    "        sigma_sig_run3 = (sigma_sig_run3).tolist()[delimiter:]\n",
    "        sigma_sig_run1.extend(sigma_sig_run3)\n",
    "\n",
    "\n",
    "\n",
    "        model = pyhf.Model(\n",
    "            {\n",
    "          \"channels\": [\n",
    "            {\n",
    "              \"name\": \"singlechannel\",\n",
    "              \"samples\": [\n",
    "                {\n",
    "                  \"name\": \"signal\",\n",
    "                  \"data\": n_sig_run1,\n",
    "                  \"modifiers\": [\n",
    "                    {\"name\": \"mu\", \"type\": \"normfactor\", \"data\": None}, #This is the scaling which is to be calculated\n",
    "                    {\"name\": \"uncorr_siguncrt\", \"type\": \"shapesys\", \"data\": sigma_sig_run1},\n",
    "                    {\"name\": \"pot_correlaated\", \"type\": \"normsys\", \"data\": {\"hi\":1.02, \"lo\":0.98}},\n",
    "                  ]\n",
    "                },\n",
    "                {\n",
    "                  \"name\": \"background\",\n",
    "                  \"data\": n_back,\n",
    "                  \"modifiers\": [\n",
    "                    {\"name\": \"uncorr_bkguncrt\", \"type\": \"shapesys\", \"data\": back_sigma},\n",
    "                    {\"name\": \"pot_correlated\", \"type\": \"normsys\", \"data\": {\"hi\":1.02, \"lo\":0.98}},\n",
    "                  ]\n",
    "                }\n",
    "              ]\n",
    "            }\n",
    "          ]\n",
    "        }\n",
    "        )\n",
    "\n",
    "        obs = n_data + model.config.auxdata\n",
    "\n",
    "        poi_values = np.linspace(0., 10., 100)\n",
    "        obs_limit, exp_limits, (scan, results) = pyhf.infer.intervals.upperlimit(\n",
    "            obs, model, poi_values, level=0.1, return_results=True)\n",
    "        \n",
    "        print(\"Printing upper limits results...\")\n",
    "        print(f\"Upper limit (obs): μ = {obs_limit:.4f}\")\n",
    "        print(f\"Upper limit (exp): μ = {exp_limits[2]:.4f}\")\n",
    "\n",
    "        obs_epsilon = (nominal_eps**2)*np.sqrt(obs_limit*factor)\n",
    "        exp_epsilon = (nominal_eps**2)*np.sqrt(exp_limits[2]*factor)\n",
    "        exp_two_down = (nominal_eps**2)*np.sqrt(exp_limits[0]*factor)\n",
    "        exp_one_down = (nominal_eps**2)*np.sqrt(exp_limits[1]*factor)\n",
    "        exp_one_up = (nominal_eps**2)*np.sqrt(exp_limits[3]*factor)\n",
    "        exp_two_up = (nominal_eps**2)*np.sqrt(exp_limits[4]*factor)\n",
    "\n",
    "        print(f\"Upper limit (obs): epsilon2 = {obs_epsilon}\")\n",
    "        print(f\"Upper limit (exp): epsilon2 = {exp_epsilon}\")\n",
    "        print(f\"Upper limit +2sigma: epsilon2 = {exp_two_up}\")\n",
    "        print(f\"Upper limit +1sigma: epsilon2 = {exp_one_up}\")\n",
    "        print(f\"Upper limit -1sigma: epsilon2 = {exp_one_down}\")\n",
    "        print(f\"Upper limit -2sigma: epsilon2 = {exp_two_down}\")\n",
    "        print(\"\\n\")\n",
    "        \n",
    "        \n",
    "        print(\"Printing CLs values for the observed value of mu (Asymptotic)...\")\n",
    "        CLs_obs_asymp, CLs_exp_asymp = pyhf.infer.hypotest(exp_limits[2], obs, model, \n",
    "                                                           return_expected_set=True)\n",
    "        \n",
    "        print(f\"Observed CLs = {CLs_obs_asymp}\")\n",
    "        print(f\"Expected CLs = {CLs_exp_asymp}\")\n",
    "        print(\"\\n\")\n",
    "        \n",
    "        print(\"Printing CLs values for the observed value of mu (Toys)...\")\n",
    "        CLs_obs_toys, CLs_exp_toys = pyhf.infer.hypotest(exp_limits[2], obs, model, \n",
    "                                                           calctype=\"toybased\",\n",
    "                                                           return_expected_set=True)\n",
    "        print(f\"Observed CLs = {CLs_obs_toys}\")\n",
    "        print(f\"Expected CLs = {CLs_exp_toys}\")\n",
    "        print(\"\\n\")\n",
    "        \n",
    "        print(f\"Ratio of both calculators = {CLs_obs_toys/CLs_obs_asymp}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
