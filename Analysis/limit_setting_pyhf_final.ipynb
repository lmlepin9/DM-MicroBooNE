{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ff150e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uproot as u\n",
    "import numpy as np\n",
    "import pyhf\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c333d47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test option\n",
    "tests = False\n",
    "\n",
    "flat_uncertainty=False\n",
    "uncertainty = 0.5\n",
    "nominal_eps = 1e-3 \n",
    "obs_eps = []\n",
    "\n",
    "# Corrections for time window and flux uncertainty\n",
    "\n",
    "fraction_outside = 1.1 \n",
    "flux_uncert = 0.22 \n",
    "\n",
    "# Array to contain expected values \n",
    "obs_eps = []\n",
    "exp_eps = []\n",
    "exp_minus_two = []\n",
    "exp_minus_one = []\n",
    "exp_plus_one = []\n",
    "exp_plus_two = []\n",
    "CLb_values = []\n",
    "\n",
    "scaling_list = [] \n",
    "dm_type = \"fermion\"\n",
    "alpha = 0.1\n",
    "ratio = \"0.6\"\n",
    "tag = \"CNN\"\n",
    "\n",
    "\n",
    "if(alpha==1.0):\n",
    "    scaling = 0.05\n",
    "else:\n",
    "    scaling = 0.05\n",
    "\n",
    "\n",
    "base_dir_run1 = \"/home/lmlepin/Desktop/dm_sets/dark_tridents_analysis/run1_samples/\"\n",
    "base_dir_run3 = \"/home/lmlepin/Desktop/dm_sets/dark_tridents_analysis/run3_samples/\"\n",
    "\n",
    "signal_dir_run1 = \"/home/lmlepin/Desktop/dm_sets/dark_tridents_analysis/run1_signal/\"\n",
    "signal_dir_run3 = \"/home/lmlepin/Desktop/dm_sets/dark_tridents_analysis/run3_signal/\"\n",
    "\n",
    "\n",
    "signal_run1 = u.open(signal_dir_run1 + dm_type + \"_ratio_\" + ratio + \"_signal_hist_run1_\" + tag + \".root\")\n",
    "signal_run3 = u.open(signal_dir_run3 + dm_type + \"_ratio_\" + ratio + \"_signal_hist_run3_\" + tag + \".root\")\n",
    "\n",
    "bkg_run1 = u.open(base_dir_run1 + \"background_hist_run1_\" + tag + \".root\")\n",
    "bkg_run3 = u.open(base_dir_run3 + \"background_hist_run3_\" + tag + \".root\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "337cd29b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Background events run1: 222.15\n",
      "Background events run3: 447.76\n",
      "\n",
      "\n",
      "Data events run1: 226.00\n",
      "Data events run3: 383.00\n"
     ]
    }
   ],
   "source": [
    "if(ratio == \"0.6\"):\n",
    "    #masses = [\"0.01\", \"0.02\", \"0.03\", \"0.04\", \"0.05\", \"0.06\", \"0.07\", \"0.08\", \"0.09\", \"0.1\", \"0.2\", \"0.3\", \"0.4\"]\n",
    "    masses = [\"0.05\"]\n",
    "else:\n",
    "    masses = [\"0.010\", \"0.020\", \"0.030\", \n",
    "              \"0.040\", \"0.050\", \"0.060\", \n",
    "              \"0.065\", \"0.070\", \"0.075\", \n",
    "              \"0.080\", \"0.085\", \"0.090\", \n",
    "              \"0.095\", \"0.100\", \"0.105\", \n",
    "              \"0.110\", \"0.115\", \"0.120\", \"0.125\"]\n",
    "    \n",
    "print(\"Background events run1: {nevts:.2f}\".format( nevts= np.sum(bkg_run1[\"bkg_total_hist\"].values())))\n",
    "print(\"Background events run3: {nevts:.2f}\".format( nevts= np.sum(bkg_run3[\"bkg_total_hist\"].values()))) \n",
    "print(\"\\n\")\n",
    "print(\"Data events run1: {nevts:.2f}\".format( nevts= np.sum(bkg_run1[\"data_hist\"].values())))\n",
    "print(\"Data events run3: {nevts:.2f}\".format( nevts= np.sum(bkg_run3[\"data_hist\"].values()))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1adda34d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing mass: 0.05\n",
      "Signal events run1: 719.79909\n",
      "Signal events run3: 1811.24287\n",
      "Scaling factor:  0.013895948257267832\n",
      "Upper limit (obs): μ = 0.9823\n",
      "Upper limit (exp): μ = 1.0235\n",
      "p-value: = 0.46\n",
      "Upper limit (obs): epsilon2 = 1.1683072656879228e-07\n",
      "Upper limit (exp): epsilon2 = 1.19260136072126e-07\n",
      "Upper limit +2sigma: epsilon2 = 1.8887470273039212e-07\n",
      "Upper limit +1sigma: epsilon2 = 1.4990247887448502e-07\n",
      "Upper limit -1sigma: epsilon2 = 9.747581000639355e-08\n",
      "Upper limit -2sigma: epsilon2 = 8.249504397010253e-08\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if(not tests):\n",
    "\n",
    "\n",
    "    delimiter=0\n",
    "    pot_uncert = 0.02 # 2% POT uncertainty \n",
    "\n",
    "\n",
    "    total_bkg = np.sum(bkg_run1[\"bkg_total_hist\"].values()) + np.sum(bkg_run3[\"bkg_total_hist\"].values())\n",
    "\n",
    "    total_run1 = np.sum(bkg_run1[\"bkg_total_hist\"].values())\n",
    "    total_run3 = np.sum(bkg_run3[\"bkg_total_hist\"].values())\n",
    "\n",
    "    n_back = bkg_run1[\"bkg_total_hist\"].values().tolist()[delimiter:]\n",
    "    n_back.extend(bkg_run3[\"bkg_total_hist\"].values().tolist()[delimiter:])\n",
    "\n",
    "\n",
    "    n_data = bkg_run1[\"data_hist\"].values().tolist()[delimiter:]\n",
    "    n_data.extend(bkg_run3[\"data_hist\"].values().tolist()[delimiter:])\n",
    "\n",
    "\n",
    "    if(flat_uncertainty):\n",
    "        back_sigma =  (bkg_run1[\"bkg_total_hist\"].values()*uncertainty).tolist()\n",
    "        back_sigma.extend((bkg_run3[\"bkg_total_hist\"].values()*uncertainty).tolist())\n",
    "    else:\n",
    "        back_sigma =  (bkg_run1[\"bkg_total_uncert\"].values()).tolist()[delimiter:]\n",
    "        back_sigma.extend((bkg_run3[\"bkg_total_uncert\"].values()).tolist()[delimiter:])\n",
    "\n",
    "    if alpha == 1.0:\n",
    "        scaling_a1 = (1./pow(0.1,3))\n",
    "    else:\n",
    "        scaling_a1 = 1. \n",
    "\n",
    "    for mass in masses:\n",
    "\n",
    "        print(\"Processing mass: \" + mass)\n",
    "        print(\"Signal events run1: {nevts:.5f}\".format( nevts= np.sum(signal_run1[\"signal_\"+ mass].values()*scaling_a1)))\n",
    "        print(\"Signal events run3: {nevts:.5f}\".format( nevts= np.sum(signal_run3[\"signal_\"+ mass].values()*scaling_a1)))\n",
    "\n",
    "        total_sig = np.sum(signal_run1[\"signal_\"+mass].values()*scaling_a1) + np.sum(signal_run3[\"signal_\"+mass].values()*scaling_a1)\n",
    "        total_sig_run1 = np.sum(signal_run1[\"signal_\"+mass].values()*scaling_a1)\n",
    "        total_sig_run3 =  np.sum(signal_run3[\"signal_\"+mass].values()*scaling_a1)\n",
    "        \n",
    "        factor = scaling*(((total_run1/total_sig_run1)+ (total_run3/total_sig_run3))/2.)\n",
    "        total_sig_adjusted = np.sum(signal_run1[\"signal_\"+mass].values()*scaling_a1*factor) + np.sum(signal_run3[\"signal_\"+mass].values()*scaling_a1*factor)\n",
    "        \n",
    "        scaling_list.append(factor)\n",
    "        \n",
    "\n",
    "        print(\"Scaling factor: \", factor)\n",
    "\n",
    "\n",
    "        # Factor scalings is introduced to have a signal strength between 0 and 10\n",
    "        # We correct by the fraction of events outside the beam window\n",
    "        n_sig_run1 = (signal_run1[\"signal_\"+mass].values()*scaling_a1*fraction_outside*factor).tolist()[delimiter:]\n",
    "        n_sig_run3 = (signal_run3[\"signal_\"+mass].values()*scaling_a1*fraction_outside*factor).tolist()[delimiter:]\n",
    "        n_sig_run1.extend(n_sig_run3)\n",
    "\n",
    "\n",
    "        # Total signal errors are in %, need to convert them to absolute errors for shapesys modifier \n",
    "        # We correct by the fraction of events outside the beam window\n",
    "        sigma_sig_run1 = (signal_run1[\"signal_total_error_\"+mass].values()/100.)*(signal_run1[\"signal_\"+mass].values()*scaling_a1*fraction_outside*factor)\n",
    "        sigma_sig_run3 = (signal_run3[\"signal_total_error_\"+mass].values()/100.)*(signal_run3[\"signal_\"+mass].values()*scaling_a1*fraction_outside*factor)\n",
    "\n",
    "\n",
    "        sigma_sig_run1 = (sigma_sig_run1).tolist()[delimiter:]\n",
    "        sigma_sig_run3 = (sigma_sig_run3).tolist()[delimiter:]\n",
    "        sigma_sig_run1.extend(sigma_sig_run3)\n",
    "\n",
    "\n",
    "\n",
    "        model = pyhf.Model(\n",
    "            {\n",
    "          \"channels\": [\n",
    "            {\n",
    "              \"name\": \"singlechannel\",\n",
    "              \"samples\": [\n",
    "                {\n",
    "                  \"name\": \"signal\",\n",
    "                  \"data\": n_sig_run1,\n",
    "                  \"modifiers\": [\n",
    "                    {\"name\": \"mu\", \"type\": \"normfactor\", \"data\": None}, #This is the scaling which is to be calculated\n",
    "                    {\"name\": \"uncorr_siguncrt\", \"type\": \"shapesys\", \"data\": sigma_sig_run1},\n",
    "                    {\"name\": \"pot_correlated\", \"type\": \"normsys\", \"data\": {\"hi\":1. + (pot_uncert), \"lo\": 1. - (pot_uncert)}},\n",
    "                    {\"name\": \"flux_correlated\", \"type\": \"normsys\", \"data\": {\"hi\":1. + flux_uncert, \"lo\": 1. -  flux_uncert}},\n",
    "                  ]\n",
    "                },\n",
    "                {\n",
    "                  \"name\": \"background\",\n",
    "                  \"data\": n_back,\n",
    "                  \"modifiers\": [\n",
    "                    {\"name\": \"uncorr_bkguncrt\", \"type\": \"shapesys\", \"data\": back_sigma},\n",
    "                    {\"name\": \"pot_correlated\", \"type\": \"normsys\", \"data\": {\"hi\":1. + pot_uncert, \"lo\": 1 - pot_uncert}},\n",
    "                  ]\n",
    "                }\n",
    "              ]\n",
    "            }\n",
    "          ]\n",
    "        }\n",
    "        )\n",
    "\n",
    "        obs = n_data + model.config.auxdata\n",
    "        poi_values = np.linspace(0., 10., 100)\n",
    "        obs_limit, exp_limits, (scan, results) = pyhf.infer.intervals.upperlimit(\n",
    "            obs, model, poi_values, level=0.1, return_results=True)\n",
    "        \n",
    "        \n",
    "                \n",
    "        CLs_value, p_values, CLs_band = pyhf.infer.hypotest(obs_limit, obs, model, \n",
    "                                                           return_expected_set=True, \n",
    "                                                           return_tail_probs=True)\n",
    "        \n",
    "        print(f\"Upper limit (obs): μ = {obs_limit:.4f}\")\n",
    "        print(f\"Upper limit (exp): μ = {exp_limits[2]:.4f}\")\n",
    "        print(f\"p-value: = {p_values[1]:.2f}\")\n",
    "\n",
    "        obs_epsilon = (nominal_eps**2)*np.sqrt(obs_limit*factor)\n",
    "        exp_epsilon = (nominal_eps**2)*np.sqrt(exp_limits[2]*factor)\n",
    "        exp_two_down = (nominal_eps**2)*np.sqrt(exp_limits[0]*factor)\n",
    "        exp_one_down = (nominal_eps**2)*np.sqrt(exp_limits[1]*factor)\n",
    "        exp_one_up = (nominal_eps**2)*np.sqrt(exp_limits[3]*factor)\n",
    "        exp_two_up = (nominal_eps**2)*np.sqrt(exp_limits[4]*factor)\n",
    "\n",
    "        obs_eps.append(obs_epsilon)\n",
    "        exp_eps.append(exp_epsilon)\n",
    "        exp_minus_two.append(exp_two_down)\n",
    "        exp_minus_one.append(exp_one_down)\n",
    "        exp_plus_one.append(exp_one_up)\n",
    "        exp_plus_two.append(exp_two_up)\n",
    "        CLb_values.append(p_values[1])\n",
    "        print(f\"Upper limit (obs): epsilon2 = {obs_epsilon}\")\n",
    "        print(f\"Upper limit (exp): epsilon2 = {exp_epsilon}\")\n",
    "        print(f\"Upper limit +2sigma: epsilon2 = {exp_two_up}\")\n",
    "        print(f\"Upper limit +1sigma: epsilon2 = {exp_one_up}\")\n",
    "        print(f\"Upper limit -1sigma: epsilon2 = {exp_one_down}\")\n",
    "        print(f\"Upper limit -2sigma: epsilon2 = {exp_two_down}\")\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aeeb32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(CLs_band)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c41a90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLs vs mu\n",
    "from matplotlib.ticker import AutoMinorLocator, MultipleLocator\n",
    "from pyhf.contrib.viz import brazil\n",
    "figure_size=(20,13)\n",
    "\n",
    "fig = plt.figure(figsize=figure_size,dpi=300)\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "for axis in ['top', 'bottom', 'left', 'right']:\n",
    "    ax.spines[axis].set_linewidth(2.5)  # change width\n",
    "    ax.spines[axis].set_color('black')    # change color\n",
    "    \n",
    "    \n",
    "ax.xaxis.set_minor_locator(AutoMinorLocator())\n",
    "ax.yaxis.set_minor_locator(AutoMinorLocator())\n",
    "ax.set_title(\"Hypothesis Test\", size=40)\n",
    "ax.set_xlim(0.1,2.0)\n",
    "#plt.grid(alpha=0.5)\n",
    "plt.tick_params(axis=\"y\", which='major', direction=\"in\",length=20,width=2.0,pad=10)\n",
    "plt.tick_params(axis=\"y\", which='minor', direction=\"in\",length=10,width=2.0,labelleft=False)\n",
    "plt.tick_params(axis=\"x\", which='major', direction=\"in\",length=20,width=2.0,pad=10)\n",
    "plt.tick_params(axis=\"x\", which='minor', direction=\"in\",length=10,width=2.0,labelbottom=False)\n",
    "plt.xticks(size=25)\n",
    "plt.yticks(size=25)\n",
    "artists = brazil.plot_results(poi_values, results,test_size=0.1, ax=ax)\n",
    "ax.set_ylabel(r'$\\mathrm{CL}_{\\mathrm{s}}$',size=40)\n",
    "ax.set_xlabel(r'$\\mu$',size=40)\n",
    "ax.legend(fontsize=35)\n",
    "plots_dir = \"/home/lmlepin/Desktop/Plots_DT_Drive/2023/sensitivity_plots/\"\n",
    "plt.savefig(plots_dir + \"CLs_scan_values.png\",bbox_inches=\"tight\")\n",
    "plt.savefig(plots_dir + \"CLs_scan_values.pdf\",bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5eb0d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if(not tests):\n",
    "\n",
    "\n",
    "    import pandas as pd\n",
    "    from matplotlib import patheffects\n",
    "\n",
    "    limits_dir = \"/home/lmlepin/Desktop/dm_sets/dark_tridents_analysis/\"\n",
    "    plots_dir = \"/home/lmlepin/Desktop/Plots_DT_Drive/2023/sensitivity_plots/\"\n",
    "    sensitivity_files = \"/home/lmlepin/Desktop/dm_sets/dark_tridents_analysis/sensitivity_files/\"\n",
    "\n",
    "    df = pd.read_csv(limits_dir + \"dark_tridents_current_limits.csv\")\n",
    "    df_b = pd.read_csv(limits_dir + \"dark_tridents_current_limits_1.csv\")\n",
    "    df_babar = pd.read_csv(limits_dir + \"babar_paper.csv\")\n",
    "    df_na = pd.read_csv(limits_dir + \"NA48_2.csv\")\n",
    "\n",
    "\n",
    "    if(ratio==\"0.6\"):\n",
    "        dp_masses = [0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1, 0.2, 0.3, 0.4]\n",
    "    else:\n",
    "        dp_masses = [0.010, 0.020, 0.030, \n",
    "              0.040, 0.050, 0.060, \n",
    "              0.065, 0.070, 0.075, \n",
    "              0.080, 0.085, 0.090, \n",
    "              0.095, 0.100, 0.105, \n",
    "              0.110, 0.115, 0.120, 0.125]\n",
    "\n",
    "    plt.figure(figsize=(17,14),dpi=300)\n",
    "    plt.axis([ 1e-2, 1, 1e-11, 1e-5])\n",
    "    plt.plot(dp_masses,exp_eps,label=r'MicroBooNE',color='red')\n",
    "    plt.plot(dp_masses,obs_eps,label=r'MicroBooNE',color='black')\n",
    "\n",
    "\n",
    "\n",
    "    if(dm_type == 'fermion' and alpha == 0.1):\n",
    "        plt.plot(df['X_LSND'],df['Y_LSND'],'-', label=r'LSND',color='green')\n",
    "        plt.fill_between(df['X_LSND'],df['Y_LSND'], 1e-5,color='green',alpha=0.2)\n",
    "        plt.plot(df['X_PLANCK'],df['Y_PLANCK'],'-',color='orange', path_effects=[patheffects.withTickedStroke(spacing=10, angle=280)])\n",
    "        plt.plot(df['X_PLANCK'],df['Y_PLANCK'],'-', label=r'Planck (fermionic DM)',color='orange')\n",
    "    elif(dm_type == 'fermion' and alpha == 1.):\n",
    "        plt.plot(df_b['X_LSND'],df_b['Y_LSND'],'-', label=r'LSND',color='green')\n",
    "        plt.fill_between(df_b['X_LSND'],df_b['Y_LSND'], 1e-5,color='green',alpha=0.2)\n",
    "        plt.plot(df_b['X_PLANCK'],df_b['Y_PLANCK'],'-',color='orange', path_effects=[patheffects.withTickedStroke(spacing=10, angle=280)])\n",
    "        plt.plot(df_b['X_PLANCK'],df_b['Y_PLANCK'],'-', label=r'Planck (fermionic DM)',color='orange')\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "\n",
    "    limit_dict = {'mass':dp_masses,'observed':obs_eps, 'epsilon_squared':exp_eps, 'two_sig_down':exp_minus_two,\n",
    "                 'one_sig_down':exp_minus_one, 'one_sig_up':exp_plus_one, 'two_sig_up':exp_plus_two,\n",
    "                 'CLb':CLb_values}\n",
    "    #df_out = pd.DataFrame.from_dict(limit_dict)\n",
    "   # df_out.to_csv(sensitivity_files + dm_type + \"_\" + tag + \"_sensitivity_alpha_\" + str(alpha) + \"_ratio_\" + ratio + \"_all_runs_full_uncert_signal_flux_0_percent.csv\")\n",
    "\n",
    "\n",
    "    #plt.plot(df['X_BD'],df['Y_BD'],'-', label=r'Beam Dump',color='mediumpurple')\n",
    "    plt.fill_between(df['X_BD'],df['Y_BD'],1e-11,color='mediumpurple',alpha=0.3)\n",
    "    #plt.plot(df_babar['X_babar'],np.square(df_babar['Y_babar']),'-', label=r'BaBar',color='blue')\n",
    "    plt.fill_between(df_babar['X_babar'],np.square(df_babar['Y_babar']), 1e-5,color='blue',alpha=0.2)\n",
    "    #plt.plot(df_na['X']*1e-3,df_na['Y'],'-', label=r'NA48/2',color='blue')\n",
    "    plt.fill_between(df_na['X']*1e-3,df_na['Y'], 1e-5,color='blue',alpha=0.3)\n",
    "    plt.yscale('log')\n",
    "    plt.xscale('log')\n",
    "    plt.ylim(1e-11,1e-5)\n",
    "    plt.xlim(1e-2,5e-1)\n",
    "    plt.legend(fontsize=20,loc=\"lower right\",shadow=True)\n",
    "    plt.xticks(size=25)\n",
    "    plt.yticks(size=25)\n",
    "    plt.title(\"Sensitivity for 7.56e20 POT           \" + dm_type + \" DM: \" + r'$\\alpha_{D}$ = ' + str(alpha), size = 30, pad=20)\n",
    "    plt.xlabel(r'$M_{A^{\\prime}}$[GeV]',size=30, labelpad=20)\n",
    "    plt.ylabel(r'$\\epsilon^2$',size=30, labelpad=20)\n",
    "    #plt.minorticks_off()\n",
    "    #plt.savefig(plots_dir + dm_type + \"_\" + tag + \"_sensitivity_alpha_\" + str(alpha) + \".png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25bfe8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison with toy-based calculator \n",
    "\n",
    "\n",
    "if(tests):\n",
    "    masses = [\"0.05\"]\n",
    "    delimiter=0\n",
    "    pot_uncert = 0.02 # 2% POT uncertainty \n",
    "\n",
    "\n",
    "    total_bkg = np.sum(bkg_run1[\"bkg_total_hist\"].values()) + np.sum(bkg_run3[\"bkg_total_hist\"].values())\n",
    "\n",
    "    total_run1 = np.sum(bkg_run1[\"bkg_total_hist\"].values())\n",
    "    total_run3 = np.sum(bkg_run3[\"bkg_total_hist\"].values())\n",
    "\n",
    "    n_back = bkg_run1[\"bkg_total_hist\"].values().tolist()[delimiter:]\n",
    "    n_back.extend(bkg_run3[\"bkg_total_hist\"].values().tolist()[delimiter:])\n",
    "\n",
    "\n",
    "    n_data = bkg_run1[\"data_hist\"].values().tolist()[delimiter:]\n",
    "    n_data.extend(bkg_run3[\"data_hist\"].values().tolist()[delimiter:])\n",
    "\n",
    "\n",
    "    if(flat_uncertainty):\n",
    "        back_sigma =  (bkg_run1[\"bkg_total_hist\"].values()*uncertainty).tolist()\n",
    "        back_sigma.extend((bkg_run3[\"bkg_total_hist\"].values()*uncertainty).tolist())\n",
    "    else:\n",
    "        back_sigma =  (bkg_run1[\"bkg_total_uncert\"].values()).tolist()[delimiter:]\n",
    "        back_sigma.extend((bkg_run3[\"bkg_total_uncert\"].values()).tolist()[delimiter:])\n",
    "\n",
    "    if alpha == 1.0:\n",
    "        scaling_a1 = (1./pow(0.1,3))\n",
    "    else:\n",
    "        scaling_a1 = 1. \n",
    "\n",
    "    for mass in masses:\n",
    "\n",
    "        print(\"Processing mass: \" + mass)\n",
    "        print(\"Signal events run1: {nevts:.5f}\".format( nevts= np.sum(signal_run1[\"signal_\"+ mass].values()*scaling_a1)))\n",
    "        print(\"Signal events run3: {nevts:.5f}\".format( nevts= np.sum(signal_run3[\"signal_\"+ mass].values()*scaling_a1)))\n",
    "\n",
    "        total_sig = np.sum(signal_run1[\"signal_\"+mass].values()*scaling_a1) + np.sum(signal_run3[\"signal_\"+mass].values()*scaling_a1)\n",
    "        total_sig_run1 = np.sum(signal_run1[\"signal_\"+mass].values()*scaling_a1)\n",
    "        total_sig_run3 =  np.sum(signal_run3[\"signal_\"+mass].values()*scaling_a1)\n",
    "        factor = scaling*(((total_run1/total_sig_run1)+ (total_run3/total_sig_run3))/2.)\n",
    "        total_sig_adjusted = np.sum(signal_run1[\"signal_\"+mass].values()*scaling_a1*factor) + np.sum(signal_run3[\"signal_\"+mass].values()*scaling_a1*factor)\n",
    "        scaling_list.append(factor)\n",
    "\n",
    "        print(\"Scaling factor: \", factor)\n",
    "\n",
    "\n",
    "        # Factor scalings is introduced to have a signal strength between 0 and 10\n",
    "        n_sig_run1 = (signal_run1[\"signal_\"+mass].values()*scaling_a1*factor).tolist()[delimiter:]\n",
    "        n_sig_run3 = (signal_run3[\"signal_\"+mass].values()*scaling_a1*factor).tolist()[delimiter:]\n",
    "        n_sig_run1.extend(n_sig_run3)\n",
    "\n",
    "\n",
    "        # Total signal errors are in %, need to convert them to absolute errors for shapesys modifier  \n",
    "        sigma_sig_run1 = (signal_run1[\"signal_total_error_\"+mass].values()/100.)*(signal_run1[\"signal_\"+mass].values()*scaling_a1*factor)\n",
    "        sigma_sig_run3 = (signal_run3[\"signal_total_error_\"+mass].values()/100.)*(signal_run3[\"signal_\"+mass].values()*scaling_a1*factor)\n",
    "\n",
    "\n",
    "        sigma_sig_run1 = (sigma_sig_run1).tolist()[delimiter:]\n",
    "        sigma_sig_run3 = (sigma_sig_run3).tolist()[delimiter:]\n",
    "        sigma_sig_run1.extend(sigma_sig_run3)\n",
    "\n",
    "\n",
    "\n",
    "        model = pyhf.Model(\n",
    "            {\n",
    "          \"channels\": [\n",
    "            {\n",
    "              \"name\": \"singlechannel\",\n",
    "              \"samples\": [\n",
    "                {\n",
    "                  \"name\": \"signal\",\n",
    "                  \"data\": n_sig_run1,\n",
    "                  \"modifiers\": [\n",
    "                    {\"name\": \"mu\", \"type\": \"normfactor\", \"data\": None}, #This is the scaling which is to be calculated\n",
    "                    {\"name\": \"uncorr_siguncrt\", \"type\": \"shapesys\", \"data\": sigma_sig_run1},\n",
    "                    {\"name\": \"pot_correlaated\", \"type\": \"normsys\", \"data\": {\"hi\":1.02, \"lo\":0.98}},\n",
    "                  ]\n",
    "                },\n",
    "                {\n",
    "                  \"name\": \"background\",\n",
    "                  \"data\": n_back,\n",
    "                  \"modifiers\": [\n",
    "                    {\"name\": \"uncorr_bkguncrt\", \"type\": \"shapesys\", \"data\": back_sigma},\n",
    "                    {\"name\": \"pot_correlated\", \"type\": \"normsys\", \"data\": {\"hi\":1.02, \"lo\":0.98}},\n",
    "                  ]\n",
    "                }\n",
    "              ]\n",
    "            }\n",
    "          ]\n",
    "        }\n",
    "        )\n",
    "\n",
    "        obs = n_data + model.config.auxdata\n",
    "\n",
    "        poi_values = np.linspace(0., 10., 100)\n",
    "        obs_limit, exp_limits, (scan, results) = pyhf.infer.intervals.upperlimit(\n",
    "            obs, model, poi_values, level=0.1, return_results=True)\n",
    "        \n",
    "        \n",
    "        print(\"Printing upper limits results...\")\n",
    "        print(f\"Upper limit (obs): μ = {obs_limit:.4f}\")\n",
    "        print(f\"Upper limit (exp): μ = {exp_limits[2]:.4f}\")\n",
    "\n",
    "        obs_epsilon = (nominal_eps**2)*np.sqrt(obs_limit*factor)\n",
    "        exp_epsilon = (nominal_eps**2)*np.sqrt(exp_limits[2]*factor)\n",
    "        exp_two_down = (nominal_eps**2)*np.sqrt(exp_limits[0]*factor)\n",
    "        exp_one_down = (nominal_eps**2)*np.sqrt(exp_limits[1]*factor)\n",
    "        exp_one_up = (nominal_eps**2)*np.sqrt(exp_limits[3]*factor)\n",
    "        exp_two_up = (nominal_eps**2)*np.sqrt(exp_limits[4]*factor)\n",
    "\n",
    "        print(f\"Upper limit (obs): epsilon2 = {obs_epsilon}\")\n",
    "        print(f\"Upper limit (exp): epsilon2 = {exp_epsilon}\")\n",
    "        print(f\"Upper limit +2sigma: epsilon2 = {exp_two_up}\")\n",
    "        print(f\"Upper limit +1sigma: epsilon2 = {exp_one_up}\")\n",
    "        print(f\"Upper limit -1sigma: epsilon2 = {exp_one_down}\")\n",
    "        print(f\"Upper limit -2sigma: epsilon2 = {exp_two_down}\")\n",
    "        print(\"\\n\")\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        print(\"Printing CLs values for the observed value of mu (Asymptotic)...\")\n",
    "        CLs_obs_asymp, CLs_exp_asymp = pyhf.infer.hypotest(exp_limits[2], obs, model, \n",
    "                                                           return_expected_set=True)\n",
    "        \n",
    "        print(f\"Observed CLs = {CLs_obs_asymp}\")\n",
    "        print(f\"Expected CLs = {CLs_exp_asymp}\")\n",
    "        print(\"\\n\")\n",
    "        '''\n",
    "        \n",
    "        print(\"Printing CLs values for the observed value of mu (Toys)...\")\n",
    "        CLs_obs_toys, CLs_exp_toys, p_values = pyhf.infer.hypotest(exp_limits[2], obs, model, \n",
    "                                                           return_expected_set=True, \n",
    "                                                           return_tail_probs=True)\n",
    "        print(f\"Observed CLs = {CLs_obs_toys}\")\n",
    "        print(f\"Expected CLs = {CLs_exp_toys}\")\n",
    "        print(f\"P-values = {p_values}\")\n",
    "        print(\"\\n\")\n",
    "        \n",
    "       # print(f\"Ratio of both calculators = {CLs_obs_toys/CLs_obs_asymp}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
